{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "716d1d3a",
   "metadata": {},
   "source": [
    "# Generation of synthetic promoters for *C. cinerea* from upstream sequences\n",
    "## Purpose\n",
    "This notebook extracts upstream sequences from the *C. cinerea* genome and trains a neural generator. It then generates synthetic promoters based on this generator.\n",
    "## Extraction of upstream sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f80299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load required libraries\n",
    "import pandas as pd\n",
    "from Bio import SeqIO, AlignIO, BiopythonParserWarning\n",
    "import shutil\n",
    "import subprocess\n",
    "import os\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import torch\n",
    "import sys, pathlib, importlib\n",
    "\n",
    "os.chdir(\"..\") #change directory to root for relative improrts\n",
    "proj_root = pathlib.Path(r\"c:\\Users\\alexa\\OneDrive - Danmarks Tekniske Universitet\\27460 - Syntesebiologi\\Exam\\27460_synthetic_promoters\").resolve()\n",
    "sys.path.insert(0, str(proj_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf8680",
   "metadata": {},
   "source": [
    "The custom function to extract upstream sequences from and annotated genome is imported from `utils` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f690282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import from utils module\n",
    "from src.utils import extract_upstream_sequences, one_hot_encode_df_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be3105a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=BiopythonParserWarning) #ignore\n",
    "\n",
    "upstream_seq_df = extract_upstream_sequences(\"genome/amutbmut.gbk\", 1000, [\"CDS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e856ae49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of upstream sequences dataframe: 16862\n",
      "first 5 rows:\n",
      "             id                                           sequence\n",
      "0  CC2G_000001  CTCCCAAAAAGCGTAAGTCCTATTCTCTTTCTACTACTATCTTTTG...\n",
      "1  CC2G_000002  CTTGGCCTTAGTGACAACACCTTGTTCGCCACTGCTCTATCTAACT...\n",
      "2  CC2G_000003  CAAGCTCTACCGGCGAAGTGATTTGCCAATTCTTCTGTTGCCGGCG...\n",
      "3  CC2G_000004  TTCAAGTGTTTTATCACAGTATCTAACATCATGACTTCACCGATGG...\n",
      "4  CC2G_000005  AGACTAGCAAAGACTATAAAACAAACGGAATACATGCGCATGATAC...\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of upstream sequences dataframe:\", len(upstream_seq_df))\n",
    "print(\"first 5 rows:\\n\", upstream_seq_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a57e7e",
   "metadata": {},
   "source": [
    "## Encoding to tensor with one-hot encoding\n",
    "We now encode the extracted sequences to a tensor in preparation for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a4353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16862, 4, 1000])\n"
     ]
    }
   ],
   "source": [
    "seq_tensor:torch.Tensor = one_hot_encode_df_to_tensor(upstream_seq_df, 1000, \"sequence\")\n",
    "print(seq_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d85a6a",
   "metadata": {},
   "source": [
    "The encoded tensor has the expected size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2375fbb",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0684ea",
   "metadata": {},
   "source": [
    "### GPU initialization and status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05cfc4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available. Device: 0 - NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "GPU forward/backward synchronized\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = torch.device('cuda')\n",
    "    print(\"CUDA available. Device:\", torch.cuda.current_device(), \"-\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    dev = torch.device('cpu')\n",
    "    print(\"CUDA not available\")\n",
    "\n",
    "if dev.type == 'cuda':\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"GPU forward/backward synchronized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb2f64a",
   "metadata": {},
   "source": [
    "### Neural network setup\n",
    "A model similar to the one used in [Synthetic-promoters-via-GAN repo](https://github.com/TokaMamdoh/Synthetic-promoters-via-GAN) is utilized. Our model is simpler and runs for around 100 epochs compared to 500 used in the original project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c4151c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import Generator, Critic, SelfAttentionLayer, Initialize_weights, GradientPenalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bfb0f9",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7955dcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen output shape: torch.Size([2, 4, 1000]) critic(gen) shape: torch.Size([2, 1])\n",
      "Epoch [1/10] Batch 1/526 Loss D: 9.9993, Loss G: 0.0114\n",
      "Epoch [1/10] Batch 16/526 Loss D: 5.7286, Loss G: 0.0609\n",
      "Epoch [1/10] Batch 31/526 Loss D: -0.6563, Loss G: 0.1016\n",
      "Epoch [1/10] Batch 46/526 Loss D: -2.1982, Loss G: 0.1298\n",
      "Epoch [1/10] Batch 61/526 Loss D: -2.9431, Loss G: 0.2925\n",
      "Epoch [1/10] Batch 76/526 Loss D: -4.3895, Loss G: 1.1785\n",
      "Epoch [1/10] Batch 91/526 Loss D: -7.4009, Loss G: 3.8680\n",
      "Epoch [1/10] Batch 106/526 Loss D: -9.5030, Loss G: 7.9252\n",
      "Epoch [1/10] Batch 121/526 Loss D: -10.3180, Loss G: 11.4068\n",
      "Epoch [1/10] Batch 136/526 Loss D: -11.8043, Loss G: 15.4467\n",
      "Epoch [1/10] Batch 151/526 Loss D: -14.7317, Loss G: 17.9009\n",
      "Epoch [1/10] Batch 166/526 Loss D: -14.7316, Loss G: 6.5726\n",
      "Epoch [1/10] Batch 181/526 Loss D: -26.1685, Loss G: 32.0439\n",
      "Epoch [1/10] Batch 196/526 Loss D: -27.8054, Loss G: 41.9845\n",
      "Epoch [1/10] Batch 211/526 Loss D: -12.5567, Loss G: 51.0472\n",
      "Epoch [1/10] Batch 226/526 Loss D: -28.3172, Loss G: 68.0898\n",
      "Epoch [1/10] Batch 241/526 Loss D: -29.4634, Loss G: 92.8336\n",
      "Epoch [1/10] Batch 256/526 Loss D: -28.2349, Loss G: 129.8864\n",
      "Epoch [1/10] Batch 271/526 Loss D: -31.9676, Loss G: 120.9232\n",
      "Epoch [1/10] Batch 286/526 Loss D: -33.6129, Loss G: 134.7950\n",
      "Epoch [1/10] Batch 301/526 Loss D: -32.9007, Loss G: 143.4130\n",
      "Epoch [1/10] Batch 316/526 Loss D: -33.3526, Loss G: 159.7359\n",
      "Epoch [1/10] Batch 331/526 Loss D: -35.2109, Loss G: 168.1871\n",
      "Epoch [1/10] Batch 346/526 Loss D: -35.3841, Loss G: 156.4276\n",
      "Epoch [1/10] Batch 361/526 Loss D: -35.1830, Loss G: 156.3218\n",
      "Epoch [1/10] Batch 376/526 Loss D: -34.8769, Loss G: 148.1551\n",
      "Epoch [1/10] Batch 391/526 Loss D: -35.3860, Loss G: 144.1968\n",
      "Epoch [1/10] Batch 406/526 Loss D: -35.6118, Loss G: 141.9893\n",
      "Epoch [1/10] Batch 421/526 Loss D: -35.5496, Loss G: 154.8991\n",
      "Epoch [1/10] Batch 436/526 Loss D: -35.5661, Loss G: 149.1785\n",
      "Epoch [1/10] Batch 451/526 Loss D: -36.1611, Loss G: 151.5660\n",
      "Epoch [1/10] Batch 466/526 Loss D: -36.1886, Loss G: 133.2404\n",
      "Epoch [1/10] Batch 481/526 Loss D: -37.0087, Loss G: 136.2007\n",
      "Epoch [1/10] Batch 496/526 Loss D: -36.7879, Loss G: 145.8208\n",
      "Epoch [1/10] Batch 511/526 Loss D: -36.9802, Loss G: 143.4714\n",
      "Epoch [1/10] Batch 526/526 Loss D: -36.9708, Loss G: 145.0931\n",
      "Epoch [2/10] Batch 1/526 Loss D: -36.5611, Loss G: 140.8071\n",
      "Epoch [2/10] Batch 16/526 Loss D: -36.4822, Loss G: 148.7973\n",
      "Epoch [2/10] Batch 31/526 Loss D: -36.8745, Loss G: 139.3045\n",
      "Epoch [2/10] Batch 46/526 Loss D: -36.9741, Loss G: 145.6096\n",
      "Epoch [2/10] Batch 61/526 Loss D: -37.1946, Loss G: 128.9113\n",
      "Epoch [2/10] Batch 76/526 Loss D: -37.5805, Loss G: 141.7886\n",
      "Epoch [2/10] Batch 91/526 Loss D: -37.0759, Loss G: 140.4102\n",
      "Epoch [2/10] Batch 106/526 Loss D: -37.7639, Loss G: 136.0002\n",
      "Epoch [2/10] Batch 121/526 Loss D: -37.9018, Loss G: 131.5585\n",
      "Epoch [2/10] Batch 136/526 Loss D: -37.2947, Loss G: 125.7576\n",
      "Epoch [2/10] Batch 151/526 Loss D: -39.1537, Loss G: 115.9921\n",
      "Epoch [2/10] Batch 166/526 Loss D: -38.2394, Loss G: 117.1741\n",
      "Epoch [2/10] Batch 181/526 Loss D: -37.7549, Loss G: 143.0449\n",
      "Epoch [2/10] Batch 196/526 Loss D: -38.5850, Loss G: 131.5978\n",
      "Epoch [2/10] Batch 211/526 Loss D: -38.9312, Loss G: 122.7041\n",
      "Epoch [2/10] Batch 226/526 Loss D: -38.6855, Loss G: 123.0431\n",
      "Epoch [2/10] Batch 241/526 Loss D: -38.5662, Loss G: 125.4206\n",
      "Epoch [2/10] Batch 256/526 Loss D: -38.0279, Loss G: 119.9070\n",
      "Epoch [2/10] Batch 271/526 Loss D: -38.4659, Loss G: 114.6446\n",
      "Epoch [2/10] Batch 286/526 Loss D: -39.2800, Loss G: 120.7431\n",
      "Epoch [2/10] Batch 301/526 Loss D: -38.5171, Loss G: 118.3269\n",
      "Epoch [2/10] Batch 316/526 Loss D: -38.7790, Loss G: 116.2014\n",
      "Epoch [2/10] Batch 331/526 Loss D: -38.5003, Loss G: 116.4804\n",
      "Epoch [2/10] Batch 346/526 Loss D: -38.8094, Loss G: 116.2874\n",
      "Epoch [2/10] Batch 361/526 Loss D: -38.9092, Loss G: 113.1475\n",
      "Epoch [2/10] Batch 376/526 Loss D: -39.2761, Loss G: 108.5901\n",
      "Epoch [2/10] Batch 391/526 Loss D: -38.2299, Loss G: 113.0686\n",
      "Epoch [2/10] Batch 406/526 Loss D: -39.1876, Loss G: 105.0889\n",
      "Epoch [2/10] Batch 421/526 Loss D: -39.1214, Loss G: 109.9729\n",
      "Epoch [2/10] Batch 436/526 Loss D: -39.7020, Loss G: 104.2688\n",
      "Epoch [2/10] Batch 451/526 Loss D: -39.2423, Loss G: 104.4864\n",
      "Epoch [2/10] Batch 466/526 Loss D: -40.4521, Loss G: 97.8958\n",
      "Epoch [2/10] Batch 481/526 Loss D: -39.6275, Loss G: 106.3558\n",
      "Epoch [2/10] Batch 496/526 Loss D: -39.8641, Loss G: 99.7740\n",
      "Epoch [2/10] Batch 511/526 Loss D: -40.0303, Loss G: 93.6356\n",
      "Epoch [2/10] Batch 526/526 Loss D: -39.4973, Loss G: 97.3274\n",
      "Epoch [3/10] Batch 1/526 Loss D: -40.2579, Loss G: 97.2573\n",
      "Epoch [3/10] Batch 16/526 Loss D: -39.7972, Loss G: 98.5676\n",
      "Epoch [3/10] Batch 31/526 Loss D: -40.0225, Loss G: 98.2377\n",
      "Epoch [3/10] Batch 46/526 Loss D: -40.0421, Loss G: 98.5762\n",
      "Epoch [3/10] Batch 61/526 Loss D: -40.1763, Loss G: 89.8604\n",
      "Epoch [3/10] Batch 76/526 Loss D: -40.2910, Loss G: 96.2288\n",
      "Epoch [3/10] Batch 91/526 Loss D: -40.3778, Loss G: 89.1205\n",
      "Epoch [3/10] Batch 106/526 Loss D: -40.6272, Loss G: 92.9290\n",
      "Epoch [3/10] Batch 121/526 Loss D: -40.2241, Loss G: 93.2558\n",
      "Epoch [3/10] Batch 136/526 Loss D: -39.8136, Loss G: 96.2225\n",
      "Epoch [3/10] Batch 151/526 Loss D: -40.3410, Loss G: 88.5902\n",
      "Epoch [3/10] Batch 166/526 Loss D: -39.7999, Loss G: 91.4754\n",
      "Epoch [3/10] Batch 181/526 Loss D: -40.6231, Loss G: 86.8132\n",
      "Epoch [3/10] Batch 196/526 Loss D: -40.3387, Loss G: 88.9658\n",
      "Epoch [3/10] Batch 211/526 Loss D: -40.5961, Loss G: 87.9868\n",
      "Epoch [3/10] Batch 226/526 Loss D: -40.6671, Loss G: 84.6472\n",
      "Epoch [3/10] Batch 241/526 Loss D: -40.8991, Loss G: 86.8726\n",
      "Epoch [3/10] Batch 256/526 Loss D: -39.9536, Loss G: 80.3968\n",
      "Epoch [3/10] Batch 271/526 Loss D: -42.1559, Loss G: 87.7528\n",
      "Epoch [3/10] Batch 286/526 Loss D: -40.1218, Loss G: 85.4349\n",
      "Epoch [3/10] Batch 301/526 Loss D: -40.4453, Loss G: 82.3858\n",
      "Epoch [3/10] Batch 316/526 Loss D: -41.8762, Loss G: 85.4663\n",
      "Epoch [3/10] Batch 331/526 Loss D: -40.3228, Loss G: 82.2849\n",
      "Epoch [3/10] Batch 346/526 Loss D: -40.7260, Loss G: 84.9089\n",
      "Epoch [3/10] Batch 361/526 Loss D: -40.4354, Loss G: 88.1902\n",
      "Epoch [3/10] Batch 376/526 Loss D: -40.6698, Loss G: 76.2806\n",
      "Epoch [3/10] Batch 391/526 Loss D: -41.2963, Loss G: 84.1571\n",
      "Epoch [3/10] Batch 406/526 Loss D: -41.2317, Loss G: 81.4234\n",
      "Epoch [3/10] Batch 421/526 Loss D: -40.7939, Loss G: 73.2355\n",
      "Epoch [3/10] Batch 436/526 Loss D: -40.7162, Loss G: 78.5811\n",
      "Epoch [3/10] Batch 451/526 Loss D: -41.2015, Loss G: 73.6363\n",
      "Epoch [3/10] Batch 466/526 Loss D: -41.5555, Loss G: 73.5358\n",
      "Epoch [3/10] Batch 481/526 Loss D: -41.8483, Loss G: 83.9972\n",
      "Epoch [3/10] Batch 496/526 Loss D: -42.0707, Loss G: 80.9499\n",
      "Epoch [3/10] Batch 511/526 Loss D: -41.7236, Loss G: 82.0720\n",
      "Epoch [3/10] Batch 526/526 Loss D: -41.3012, Loss G: 82.8410\n",
      "Epoch [4/10] Batch 1/526 Loss D: -41.3261, Loss G: 84.1040\n",
      "Epoch [4/10] Batch 16/526 Loss D: -41.4046, Loss G: 80.4417\n",
      "Epoch [4/10] Batch 31/526 Loss D: -41.1519, Loss G: 79.4969\n",
      "Epoch [4/10] Batch 46/526 Loss D: -41.6309, Loss G: 75.2884\n",
      "Epoch [4/10] Batch 61/526 Loss D: -41.6591, Loss G: 76.3167\n",
      "Epoch [4/10] Batch 76/526 Loss D: -41.2428, Loss G: 80.3132\n",
      "Epoch [4/10] Batch 91/526 Loss D: -41.5923, Loss G: 71.6095\n",
      "Epoch [4/10] Batch 106/526 Loss D: -42.1825, Loss G: 80.0109\n",
      "Epoch [4/10] Batch 121/526 Loss D: -41.3571, Loss G: 71.7426\n",
      "Epoch [4/10] Batch 136/526 Loss D: -41.7338, Loss G: 73.1060\n",
      "Epoch [4/10] Batch 151/526 Loss D: -41.4740, Loss G: 77.6839\n",
      "Epoch [4/10] Batch 166/526 Loss D: -42.0356, Loss G: 83.4275\n",
      "Epoch [4/10] Batch 181/526 Loss D: -41.3518, Loss G: 70.9467\n",
      "Epoch [4/10] Batch 196/526 Loss D: -41.3207, Loss G: 67.4449\n",
      "Epoch [4/10] Batch 211/526 Loss D: -41.6277, Loss G: 75.3708\n",
      "Epoch [4/10] Batch 226/526 Loss D: -40.8296, Loss G: 76.9575\n",
      "Epoch [4/10] Batch 241/526 Loss D: -41.4173, Loss G: 71.5479\n",
      "Epoch [4/10] Batch 256/526 Loss D: -41.3851, Loss G: 73.2245\n",
      "Epoch [4/10] Batch 271/526 Loss D: -42.4810, Loss G: 68.1911\n",
      "Epoch [4/10] Batch 286/526 Loss D: -42.1073, Loss G: 72.6669\n",
      "Epoch [4/10] Batch 301/526 Loss D: -41.9092, Loss G: 68.9581\n",
      "Epoch [4/10] Batch 316/526 Loss D: -41.5247, Loss G: 71.9020\n",
      "Epoch [4/10] Batch 331/526 Loss D: -42.0414, Loss G: 73.0122\n",
      "Epoch [4/10] Batch 346/526 Loss D: -41.9685, Loss G: 63.8334\n",
      "Epoch [4/10] Batch 361/526 Loss D: -41.9206, Loss G: 66.1379\n",
      "Epoch [4/10] Batch 376/526 Loss D: -41.9430, Loss G: 73.3293\n",
      "Epoch [4/10] Batch 391/526 Loss D: -41.6816, Loss G: 61.6284\n",
      "Epoch [4/10] Batch 406/526 Loss D: -41.1762, Loss G: 69.7931\n",
      "Epoch [4/10] Batch 421/526 Loss D: -42.4887, Loss G: 67.6150\n",
      "Epoch [4/10] Batch 436/526 Loss D: -41.6118, Loss G: 65.3779\n",
      "Epoch [4/10] Batch 451/526 Loss D: -41.6271, Loss G: 63.4439\n",
      "Epoch [4/10] Batch 466/526 Loss D: -40.6451, Loss G: 59.5516\n",
      "Epoch [4/10] Batch 481/526 Loss D: -41.7033, Loss G: 73.9824\n",
      "Epoch [4/10] Batch 496/526 Loss D: -41.6852, Loss G: 66.6728\n",
      "Epoch [4/10] Batch 511/526 Loss D: -41.9578, Loss G: 65.2623\n",
      "Epoch [4/10] Batch 526/526 Loss D: -41.9936, Loss G: 66.5383\n",
      "Epoch [5/10] Batch 1/526 Loss D: -42.6151, Loss G: 68.2298\n",
      "Epoch [5/10] Batch 16/526 Loss D: -42.2828, Loss G: 65.5584\n",
      "Epoch [5/10] Batch 31/526 Loss D: -41.8041, Loss G: 71.2523\n",
      "Epoch [5/10] Batch 46/526 Loss D: -42.1048, Loss G: 68.4084\n",
      "Epoch [5/10] Batch 61/526 Loss D: -42.9787, Loss G: 72.4855\n",
      "Epoch [5/10] Batch 76/526 Loss D: -42.2158, Loss G: 69.6851\n",
      "Epoch [5/10] Batch 91/526 Loss D: -42.5774, Loss G: 67.9897\n",
      "Epoch [5/10] Batch 106/526 Loss D: -41.9480, Loss G: 65.6956\n",
      "Epoch [5/10] Batch 121/526 Loss D: -41.7044, Loss G: 66.4519\n",
      "Epoch [5/10] Batch 136/526 Loss D: -41.6709, Loss G: 63.9005\n",
      "Epoch [5/10] Batch 151/526 Loss D: -41.9783, Loss G: 67.4564\n",
      "Epoch [5/10] Batch 166/526 Loss D: -41.0780, Loss G: 73.3806\n",
      "Epoch [5/10] Batch 181/526 Loss D: -42.0258, Loss G: 63.1084\n",
      "Epoch [5/10] Batch 196/526 Loss D: -41.5371, Loss G: 64.0520\n",
      "Epoch [5/10] Batch 211/526 Loss D: -42.5788, Loss G: 61.7640\n",
      "Epoch [5/10] Batch 226/526 Loss D: -43.4595, Loss G: 66.3737\n",
      "Epoch [5/10] Batch 241/526 Loss D: -42.0872, Loss G: 59.5858\n",
      "Epoch [5/10] Batch 256/526 Loss D: -40.1714, Loss G: 58.8677\n",
      "Epoch [5/10] Batch 271/526 Loss D: -42.7999, Loss G: 67.0889\n",
      "Epoch [5/10] Batch 286/526 Loss D: -42.2933, Loss G: 64.2498\n",
      "Epoch [5/10] Batch 301/526 Loss D: -42.6322, Loss G: 60.7860\n",
      "Epoch [5/10] Batch 316/526 Loss D: -42.4323, Loss G: 65.0665\n",
      "Epoch [5/10] Batch 331/526 Loss D: -42.4231, Loss G: 65.0242\n",
      "Epoch [5/10] Batch 346/526 Loss D: -43.6205, Loss G: 66.4490\n",
      "Epoch [5/10] Batch 361/526 Loss D: -42.5008, Loss G: 63.6675\n",
      "Epoch [5/10] Batch 376/526 Loss D: -42.6566, Loss G: 66.7487\n",
      "Epoch [5/10] Batch 391/526 Loss D: -41.5498, Loss G: 64.9956\n",
      "Epoch [5/10] Batch 406/526 Loss D: -42.7892, Loss G: 66.9187\n",
      "Epoch [5/10] Batch 421/526 Loss D: -42.6081, Loss G: 62.6179\n",
      "Epoch [5/10] Batch 436/526 Loss D: -42.9313, Loss G: 70.0684\n",
      "Epoch [5/10] Batch 451/526 Loss D: -42.1925, Loss G: 66.1274\n",
      "Epoch [5/10] Batch 466/526 Loss D: -42.2954, Loss G: 73.7037\n",
      "Epoch [5/10] Batch 481/526 Loss D: -42.6094, Loss G: 62.4178\n",
      "Epoch [5/10] Batch 496/526 Loss D: -42.2075, Loss G: 64.8304\n",
      "Epoch [5/10] Batch 511/526 Loss D: -42.8989, Loss G: 63.8599\n",
      "Epoch [5/10] Batch 526/526 Loss D: -42.9719, Loss G: 61.8366\n",
      "Epoch [6/10] Batch 1/526 Loss D: -42.8618, Loss G: 62.0070\n",
      "Epoch [6/10] Batch 16/526 Loss D: -42.0851, Loss G: 64.1008\n",
      "Epoch [6/10] Batch 31/526 Loss D: -43.1930, Loss G: 67.7530\n",
      "Epoch [6/10] Batch 46/526 Loss D: -43.0747, Loss G: 65.2102\n",
      "Epoch [6/10] Batch 61/526 Loss D: -44.0853, Loss G: 63.6002\n",
      "Epoch [6/10] Batch 76/526 Loss D: -42.7377, Loss G: 65.1476\n",
      "Epoch [6/10] Batch 91/526 Loss D: -42.1483, Loss G: 59.7307\n",
      "Epoch [6/10] Batch 106/526 Loss D: -43.3473, Loss G: 63.9365\n",
      "Epoch [6/10] Batch 121/526 Loss D: -43.7336, Loss G: 64.8923\n",
      "Epoch [6/10] Batch 136/526 Loss D: -43.4139, Loss G: 65.3958\n",
      "Epoch [6/10] Batch 151/526 Loss D: -42.7481, Loss G: 68.7671\n",
      "Epoch [6/10] Batch 166/526 Loss D: -42.3855, Loss G: 68.5046\n",
      "Epoch [6/10] Batch 181/526 Loss D: -43.1319, Loss G: 61.4268\n",
      "Epoch [6/10] Batch 196/526 Loss D: -43.4146, Loss G: 62.0698\n",
      "Epoch [6/10] Batch 211/526 Loss D: -43.4082, Loss G: 62.9625\n",
      "Epoch [6/10] Batch 226/526 Loss D: -44.0019, Loss G: 64.3007\n",
      "Epoch [6/10] Batch 241/526 Loss D: -44.0768, Loss G: 65.4814\n",
      "Epoch [6/10] Batch 256/526 Loss D: -43.7294, Loss G: 64.9326\n",
      "Epoch [6/10] Batch 271/526 Loss D: -42.7251, Loss G: 66.0481\n",
      "Epoch [6/10] Batch 286/526 Loss D: -43.4086, Loss G: 63.0090\n",
      "Epoch [6/10] Batch 301/526 Loss D: -43.7132, Loss G: 63.4601\n",
      "Epoch [6/10] Batch 316/526 Loss D: -43.0930, Loss G: 62.0108\n",
      "Epoch [6/10] Batch 331/526 Loss D: -44.0198, Loss G: 60.7234\n",
      "Epoch [6/10] Batch 346/526 Loss D: -43.9437, Loss G: 65.5616\n",
      "Epoch [6/10] Batch 361/526 Loss D: -43.6571, Loss G: 63.6767\n",
      "Epoch [6/10] Batch 376/526 Loss D: -42.8056, Loss G: 68.2786\n",
      "Epoch [6/10] Batch 391/526 Loss D: -44.2906, Loss G: 67.7871\n",
      "Epoch [6/10] Batch 406/526 Loss D: -42.6895, Loss G: 64.1756\n",
      "Epoch [6/10] Batch 421/526 Loss D: -43.7718, Loss G: 61.7051\n",
      "Epoch [6/10] Batch 436/526 Loss D: -43.9858, Loss G: 68.5077\n",
      "Epoch [6/10] Batch 451/526 Loss D: -43.9297, Loss G: 59.6636\n",
      "Epoch [6/10] Batch 466/526 Loss D: -42.8845, Loss G: 67.4578\n",
      "Epoch [6/10] Batch 481/526 Loss D: -43.4273, Loss G: 65.8973\n",
      "Epoch [6/10] Batch 496/526 Loss D: -43.1895, Loss G: 64.7797\n",
      "Epoch [6/10] Batch 511/526 Loss D: -43.5918, Loss G: 71.8527\n",
      "Epoch [6/10] Batch 526/526 Loss D: -43.8031, Loss G: 59.9092\n",
      "Epoch [7/10] Batch 1/526 Loss D: -42.4567, Loss G: 64.1108\n",
      "Epoch [7/10] Batch 16/526 Loss D: -43.6964, Loss G: 61.5229\n",
      "Epoch [7/10] Batch 31/526 Loss D: -44.3376, Loss G: 66.0045\n",
      "Epoch [7/10] Batch 46/526 Loss D: -45.5617, Loss G: 64.2514\n",
      "Epoch [7/10] Batch 61/526 Loss D: -40.7436, Loss G: 57.0637\n",
      "Epoch [7/10] Batch 76/526 Loss D: -43.6600, Loss G: 63.9726\n",
      "Epoch [7/10] Batch 91/526 Loss D: -43.5670, Loss G: 62.8716\n",
      "Epoch [7/10] Batch 106/526 Loss D: -42.6198, Loss G: 66.5289\n",
      "Epoch [7/10] Batch 121/526 Loss D: -43.0938, Loss G: 64.7706\n",
      "Epoch [7/10] Batch 136/526 Loss D: -42.9187, Loss G: 68.3452\n",
      "Epoch [7/10] Batch 151/526 Loss D: -43.2165, Loss G: 64.2763\n",
      "Epoch [7/10] Batch 166/526 Loss D: -43.0674, Loss G: 64.0677\n",
      "Epoch [7/10] Batch 181/526 Loss D: -43.9444, Loss G: 62.6103\n",
      "Epoch [7/10] Batch 196/526 Loss D: -44.0358, Loss G: 62.1893\n",
      "Epoch [7/10] Batch 211/526 Loss D: -42.5694, Loss G: 55.5930\n",
      "Epoch [7/10] Batch 226/526 Loss D: -43.3220, Loss G: 63.6416\n",
      "Epoch [7/10] Batch 241/526 Loss D: -44.7354, Loss G: 61.8555\n",
      "Epoch [7/10] Batch 256/526 Loss D: -43.3837, Loss G: 64.5725\n",
      "Epoch [7/10] Batch 271/526 Loss D: -43.7401, Loss G: 70.9586\n",
      "Epoch [7/10] Batch 286/526 Loss D: -43.9200, Loss G: 64.2473\n",
      "Epoch [7/10] Batch 301/526 Loss D: -44.0072, Loss G: 66.4757\n",
      "Epoch [7/10] Batch 316/526 Loss D: -43.7364, Loss G: 65.2537\n",
      "Epoch [7/10] Batch 331/526 Loss D: -44.2899, Loss G: 68.3238\n",
      "Epoch [7/10] Batch 346/526 Loss D: -44.2303, Loss G: 64.9322\n",
      "Epoch [7/10] Batch 361/526 Loss D: -43.4714, Loss G: 60.2926\n",
      "Epoch [7/10] Batch 376/526 Loss D: -44.1379, Loss G: 60.5486\n",
      "Epoch [7/10] Batch 391/526 Loss D: -44.4060, Loss G: 67.2144\n",
      "Epoch [7/10] Batch 406/526 Loss D: -43.8489, Loss G: 62.6631\n",
      "Epoch [7/10] Batch 421/526 Loss D: -44.9820, Loss G: 61.9537\n",
      "Epoch [7/10] Batch 436/526 Loss D: -44.3050, Loss G: 59.7692\n",
      "Epoch [7/10] Batch 451/526 Loss D: -44.9501, Loss G: 55.4896\n",
      "Epoch [7/10] Batch 466/526 Loss D: -42.7329, Loss G: 65.0733\n",
      "Epoch [7/10] Batch 481/526 Loss D: -44.0686, Loss G: 58.6310\n",
      "Epoch [7/10] Batch 496/526 Loss D: -43.9898, Loss G: 56.7370\n",
      "Epoch [7/10] Batch 511/526 Loss D: -44.0685, Loss G: 61.6932\n",
      "Epoch [7/10] Batch 526/526 Loss D: -43.9835, Loss G: 59.6291\n",
      "Epoch [8/10] Batch 1/526 Loss D: -43.8298, Loss G: 64.2537\n",
      "Epoch [8/10] Batch 16/526 Loss D: -44.5743, Loss G: 59.7715\n",
      "Epoch [8/10] Batch 31/526 Loss D: -44.2050, Loss G: 55.8746\n",
      "Epoch [8/10] Batch 46/526 Loss D: -43.3690, Loss G: 54.6713\n",
      "Epoch [8/10] Batch 61/526 Loss D: -43.9652, Loss G: 59.1731\n",
      "Epoch [8/10] Batch 76/526 Loss D: -42.9187, Loss G: 61.6325\n",
      "Epoch [8/10] Batch 91/526 Loss D: -43.8137, Loss G: 55.7227\n",
      "Epoch [8/10] Batch 106/526 Loss D: -44.0747, Loss G: 60.5089\n",
      "Epoch [8/10] Batch 121/526 Loss D: -44.8790, Loss G: 56.5733\n",
      "Epoch [8/10] Batch 136/526 Loss D: -44.1201, Loss G: 61.6527\n",
      "Epoch [8/10] Batch 151/526 Loss D: -42.6855, Loss G: 55.0404\n",
      "Epoch [8/10] Batch 166/526 Loss D: -44.0488, Loss G: 60.6026\n",
      "Epoch [8/10] Batch 181/526 Loss D: -43.7204, Loss G: 59.5274\n",
      "Epoch [8/10] Batch 196/526 Loss D: -43.1159, Loss G: 60.6097\n",
      "Epoch [8/10] Batch 211/526 Loss D: -44.0172, Loss G: 57.8308\n",
      "Epoch [8/10] Batch 226/526 Loss D: -44.1231, Loss G: 58.3034\n",
      "Epoch [8/10] Batch 241/526 Loss D: -42.5477, Loss G: 51.2212\n",
      "Epoch [8/10] Batch 256/526 Loss D: -43.8551, Loss G: 57.6655\n",
      "Epoch [8/10] Batch 271/526 Loss D: -45.3837, Loss G: 52.6386\n",
      "Epoch [8/10] Batch 286/526 Loss D: -43.1264, Loss G: 62.1517\n",
      "Epoch [8/10] Batch 301/526 Loss D: -43.7447, Loss G: 54.3400\n",
      "Epoch [8/10] Batch 316/526 Loss D: -44.2855, Loss G: 54.8583\n",
      "Epoch [8/10] Batch 331/526 Loss D: -44.5066, Loss G: 57.9865\n",
      "Epoch [8/10] Batch 346/526 Loss D: -44.4307, Loss G: 54.9502\n",
      "Epoch [8/10] Batch 361/526 Loss D: -43.8353, Loss G: 57.0061\n",
      "Epoch [8/10] Batch 376/526 Loss D: -44.7317, Loss G: 54.9859\n",
      "Epoch [8/10] Batch 391/526 Loss D: -45.5164, Loss G: 54.1042\n",
      "Epoch [8/10] Batch 406/526 Loss D: -42.3038, Loss G: 54.7751\n",
      "Epoch [8/10] Batch 421/526 Loss D: -45.0968, Loss G: 57.9646\n",
      "Epoch [8/10] Batch 436/526 Loss D: -45.0084, Loss G: 58.7446\n",
      "Epoch [8/10] Batch 451/526 Loss D: -45.0046, Loss G: 59.7849\n",
      "Epoch [8/10] Batch 466/526 Loss D: -45.5296, Loss G: 52.8275\n",
      "Epoch [8/10] Batch 481/526 Loss D: -43.4031, Loss G: 53.2355\n",
      "Epoch [8/10] Batch 496/526 Loss D: -43.6662, Loss G: 54.9896\n",
      "Epoch [8/10] Batch 511/526 Loss D: -44.5850, Loss G: 54.8519\n",
      "Epoch [8/10] Batch 526/526 Loss D: -43.6512, Loss G: 49.3282\n",
      "Epoch [9/10] Batch 1/526 Loss D: -44.2235, Loss G: 58.2050\n",
      "Epoch [9/10] Batch 16/526 Loss D: -43.5139, Loss G: 55.8014\n",
      "Epoch [9/10] Batch 31/526 Loss D: -43.7941, Loss G: 59.7197\n",
      "Epoch [9/10] Batch 46/526 Loss D: -44.4456, Loss G: 60.5198\n",
      "Epoch [9/10] Batch 61/526 Loss D: -43.7241, Loss G: 62.5202\n",
      "Epoch [9/10] Batch 76/526 Loss D: -44.1592, Loss G: 57.6967\n",
      "Epoch [9/10] Batch 91/526 Loss D: -44.3769, Loss G: 55.5953\n",
      "Epoch [9/10] Batch 106/526 Loss D: -43.7090, Loss G: 56.8519\n",
      "Epoch [9/10] Batch 121/526 Loss D: -41.5005, Loss G: 54.6015\n",
      "Epoch [9/10] Batch 136/526 Loss D: -43.9410, Loss G: 57.8246\n",
      "Epoch [9/10] Batch 151/526 Loss D: -44.1429, Loss G: 58.6128\n",
      "Epoch [9/10] Batch 166/526 Loss D: -44.4681, Loss G: 54.7674\n",
      "Epoch [9/10] Batch 181/526 Loss D: -44.4701, Loss G: 65.6283\n",
      "Epoch [9/10] Batch 196/526 Loss D: -44.4928, Loss G: 61.4901\n",
      "Epoch [9/10] Batch 211/526 Loss D: -44.9769, Loss G: 57.6820\n",
      "Epoch [9/10] Batch 226/526 Loss D: -44.7432, Loss G: 64.5795\n",
      "Epoch [9/10] Batch 241/526 Loss D: -45.1156, Loss G: 55.8151\n",
      "Epoch [9/10] Batch 256/526 Loss D: -44.8924, Loss G: 54.2844\n",
      "Epoch [9/10] Batch 271/526 Loss D: -44.2708, Loss G: 54.2761\n",
      "Epoch [9/10] Batch 286/526 Loss D: -44.6637, Loss G: 60.5122\n",
      "Epoch [9/10] Batch 301/526 Loss D: -43.9381, Loss G: 60.7213\n",
      "Epoch [9/10] Batch 316/526 Loss D: -43.4372, Loss G: 57.5394\n",
      "Epoch [9/10] Batch 331/526 Loss D: -43.8244, Loss G: 60.4875\n",
      "Epoch [9/10] Batch 346/526 Loss D: -44.5528, Loss G: 53.6197\n",
      "Epoch [9/10] Batch 361/526 Loss D: -44.8073, Loss G: 54.4851\n",
      "Epoch [9/10] Batch 376/526 Loss D: -44.7249, Loss G: 57.2587\n",
      "Epoch [9/10] Batch 391/526 Loss D: -44.8663, Loss G: 59.9917\n",
      "Epoch [9/10] Batch 406/526 Loss D: -44.6023, Loss G: 58.3949\n",
      "Epoch [9/10] Batch 421/526 Loss D: -43.7544, Loss G: 59.0917\n",
      "Epoch [9/10] Batch 436/526 Loss D: -43.3965, Loss G: 53.9347\n",
      "Epoch [9/10] Batch 451/526 Loss D: -44.8571, Loss G: 58.1834\n",
      "Epoch [9/10] Batch 466/526 Loss D: -44.5891, Loss G: 56.9164\n",
      "Epoch [9/10] Batch 481/526 Loss D: -44.6574, Loss G: 50.0192\n",
      "Epoch [9/10] Batch 496/526 Loss D: -42.9588, Loss G: 59.3360\n",
      "Epoch [9/10] Batch 511/526 Loss D: -44.9129, Loss G: 55.3518\n",
      "Epoch [9/10] Batch 526/526 Loss D: -44.7304, Loss G: 56.8848\n",
      "Epoch [10/10] Batch 1/526 Loss D: -44.4464, Loss G: 56.2887\n",
      "Epoch [10/10] Batch 16/526 Loss D: -44.2775, Loss G: 54.4675\n",
      "Epoch [10/10] Batch 31/526 Loss D: -44.7025, Loss G: 54.5488\n",
      "Epoch [10/10] Batch 46/526 Loss D: -42.4552, Loss G: 54.8137\n",
      "Epoch [10/10] Batch 61/526 Loss D: -44.1018, Loss G: 54.9085\n",
      "Epoch [10/10] Batch 76/526 Loss D: -44.6251, Loss G: 51.6949\n",
      "Epoch [10/10] Batch 91/526 Loss D: -44.8535, Loss G: 56.5920\n",
      "Epoch [10/10] Batch 106/526 Loss D: -43.5144, Loss G: 58.3120\n",
      "Epoch [10/10] Batch 121/526 Loss D: -44.0161, Loss G: 60.7648\n",
      "Epoch [10/10] Batch 136/526 Loss D: -45.1759, Loss G: 55.1394\n",
      "Epoch [10/10] Batch 151/526 Loss D: -44.5251, Loss G: 54.5038\n",
      "Epoch [10/10] Batch 166/526 Loss D: -44.2858, Loss G: 54.5528\n",
      "Epoch [10/10] Batch 181/526 Loss D: -43.1059, Loss G: 50.7160\n",
      "Epoch [10/10] Batch 196/526 Loss D: -44.0981, Loss G: 59.9150\n",
      "Epoch [10/10] Batch 211/526 Loss D: -45.4752, Loss G: 58.1716\n",
      "Epoch [10/10] Batch 226/526 Loss D: -44.3579, Loss G: 60.3479\n",
      "Epoch [10/10] Batch 241/526 Loss D: -44.2371, Loss G: 56.7152\n",
      "Epoch [10/10] Batch 256/526 Loss D: -45.0471, Loss G: 66.4507\n",
      "Epoch [10/10] Batch 271/526 Loss D: -45.1227, Loss G: 65.0957\n",
      "Epoch [10/10] Batch 286/526 Loss D: -44.6635, Loss G: 56.9579\n",
      "Epoch [10/10] Batch 301/526 Loss D: -44.4702, Loss G: 60.3886\n",
      "Epoch [10/10] Batch 316/526 Loss D: -43.8928, Loss G: 62.3804\n",
      "Epoch [10/10] Batch 331/526 Loss D: -43.7589, Loss G: 60.5416\n",
      "Epoch [10/10] Batch 346/526 Loss D: -44.6591, Loss G: 60.5068\n",
      "Epoch [10/10] Batch 361/526 Loss D: -44.9105, Loss G: 60.6887\n",
      "Epoch [10/10] Batch 376/526 Loss D: -44.2494, Loss G: 58.2069\n",
      "Epoch [10/10] Batch 391/526 Loss D: -43.9347, Loss G: 59.8571\n",
      "Epoch [10/10] Batch 406/526 Loss D: -44.5943, Loss G: 56.2703\n",
      "Epoch [10/10] Batch 421/526 Loss D: -42.5235, Loss G: 52.1847\n",
      "Epoch [10/10] Batch 436/526 Loss D: -44.3421, Loss G: 54.8298\n",
      "Epoch [10/10] Batch 451/526 Loss D: -44.4555, Loss G: 59.7460\n",
      "Epoch [10/10] Batch 466/526 Loss D: -42.9974, Loss G: 55.5518\n",
      "Epoch [10/10] Batch 481/526 Loss D: -44.7179, Loss G: 60.3165\n",
      "Epoch [10/10] Batch 496/526 Loss D: -43.0398, Loss G: 56.5937\n",
      "Epoch [10/10] Batch 511/526 Loss D: -43.1720, Loss G: 52.4086\n",
      "Epoch [10/10] Batch 526/526 Loss D: -44.4287, Loss G: 58.0630\n"
     ]
    }
   ],
   "source": [
    "log_interval = 50 # for logging to check for convergence\n",
    "loss_critic_history = []\n",
    "loss_gen_history = []\n",
    "loss_epoch_history = []\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class WGANGPLossD:\n",
    "    @staticmethod\n",
    "    def Wasserstein(critic_real, critic_fake):\n",
    "        # critic_real, critic_fake are 1D tensors (batch of scores)\n",
    "        # WGAN critic loss = E[fake] - E[real]\n",
    "        return critic_fake.mean() - critic_real.mean()\n",
    "\n",
    "class WGANGPLossG:\n",
    "    @staticmethod\n",
    "    def Wasserstein(critic_fake):\n",
    "        # generator loss = -E[critic(fake)]\n",
    "        return -critic_fake.mean()\n",
    "\n",
    "# instantiate used names\n",
    "lossD = WGANGPLossD()\n",
    "lossG = WGANGPLossG()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "assert 'seq_tensor' in globals(), \"seq_tensor not found; run encoding cell\"\n",
    "seq_channels = int(seq_tensor.shape[1])\n",
    "seq_len = int(seq_tensor.shape[2])\n",
    "\n",
    "Batch_size = globals().get('Batch_size', 32)\n",
    "noise_dim = globals().get('noise_dim', 100)\n",
    "features_gen = globals().get('features_gen', 64)\n",
    "features_critic = globals().get('features_cirtic', globals().get('features_critic', 64))\n",
    "num_epochs = globals().get('num_epochs', 10)\n",
    "Critic_iterations = globals().get('Critic_iterations', 5)\n",
    "Learning_rate = globals().get('Learning_rate', 1e-4)\n",
    "Lamda_GP = globals().get('Lamda_GP', 10.0)\n",
    "\n",
    "assert 'lossD' in globals() and 'lossG' in globals(), \"Define lossD and lossG before training\"\n",
    "\n",
    "dataloader = DataLoader(TensorDataset(seq_tensor), batch_size=Batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "gen = Generator(noise_dim, features_gen, SelfAttentionLayer, out_channels=4, target_len=seq_len).to(device)\n",
    "critic = Critic(seq_channels, features_critic, SelfAttentionLayer).to(device)\n",
    "\n",
    "Initialize_weights(gen)\n",
    "Initialize_weights(critic)\n",
    "\n",
    "opt_gen = torch.optim.Adam(gen.parameters(), lr=Learning_rate, betas=(0.0, 0.9))\n",
    "opt_critic = torch.optim.Adam(critic.parameters(), lr=Learning_rate, betas=(0.0, 0.9))\n",
    "\n",
    "def ensure_length_match(fake, real):\n",
    "    if fake.size(2) != real.size(2):\n",
    "        fake = torch.nn.functional.interpolate(fake, size=real.size(2), mode='linear', align_corners=False)\n",
    "    return fake\n",
    "\n",
    "def GradientPenalty(critic, real, fake, device):\n",
    "    B = real.size(0)\n",
    "    alpha = torch.rand(B, 1, 1, device=device)\n",
    "    alpha = alpha.expand_as(real)\n",
    "    interpolated = (alpha * real + (1 - alpha) * fake).requires_grad_(True)\n",
    "    mixed_scores = critic(interpolated)\n",
    "    scalar_score = mixed_scores.sum()\n",
    "    grad = torch.autograd.grad(outputs=scalar_score, inputs=interpolated,\n",
    "                               create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    grad = grad.view(B, -1)\n",
    "    grad_norm = grad.norm(2, dim=1)\n",
    "    return ((grad_norm - 1) ** 2).mean()\n",
    "\n",
    "# quick forward sanity check\n",
    "with torch.no_grad():\n",
    "    test_noise = torch.randn(2, noise_dim, 1, device=device)\n",
    "    test_fake = gen(test_noise)\n",
    "print(\"gen output shape:\", test_fake.shape, \"critic(gen) shape:\", critic(test_fake).shape)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real_batch,) in enumerate(dataloader):\n",
    "        real = real_batch.to(device).float()\n",
    "\n",
    "        # Train critic multiple times\n",
    "        for _ in range(Critic_iterations):\n",
    "            noise = torch.randn(real.size(0), noise_dim, 1, device=device)\n",
    "            fake = gen(noise)\n",
    "            fake = ensure_length_match(fake, real)\n",
    "\n",
    "            critic_real = critic(real).reshape(-1)\n",
    "            critic_fake = critic(fake.detach()).reshape(-1)\n",
    "\n",
    "            gp = GradientPenalty(critic, real, fake.detach(), device)\n",
    "            loss_critic = lossD.Wasserstein(critic_real, critic_fake) + Lamda_GP * gp\n",
    "\n",
    "            opt_critic.zero_grad()\n",
    "            loss_critic.backward()\n",
    "            opt_critic.step()\n",
    "\n",
    "        # Train generator once\n",
    "        noise = torch.randn(real.size(0), noise_dim, 1, device=device)\n",
    "        fake = gen(noise)\n",
    "        fake = ensure_length_match(fake, real)\n",
    "\n",
    "        critic_fake_for_gen = critic(fake).reshape(-1)\n",
    "        loss_gen = lossG.Wasserstein(critic_fake_for_gen)\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            loss_critic_history.append(loss_critic.item())\n",
    "            loss_gen_history.append(loss_gen.item())\n",
    "            loss_epoch_history.append(epoch)\n",
    "        if batch_idx % 526 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch {batch_idx+1}/{len(dataloader)} \"\n",
    "                  f\"Loss D: {loss_critic.item():.4f}, Loss G: {loss_gen.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ede0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model and loss history for analysis\n",
    "torch.save({\n",
    "    \"epoch\": epoch,\n",
    "    \"gen_state_dict\": gen.state_dict(),\n",
    "    \"critic_state_dict\": critic.state_dict(),\n",
    "    \"opt_gen_state_dict\": opt_gen.state_dict(),\n",
    "    \"opt_critic_state_dict\": opt_critic.state_dict(),\n",
    "    \"loss_gen\": loss_gen.item(),\n",
    "    \"loss_critic\": loss_critic.item(),\n",
    "    \"loss_gen_history\": loss_gen_history,\n",
    "    \"loss_critic_history\": loss_critic_history,\n",
    "    \"loss_epoch_history\": loss_epoch_history\n",
    "}, \"models/wgan_gp_100_epochs.pth\")\n",
    "\n",
    "df_losses = pd.DataFrame({\n",
    "    \"epoch\": loss_epoch_history,\n",
    "    \"loss_critic\": loss_critic_history,\n",
    "    \"loss_gen\": loss_gen_history\n",
    "})\n",
    "print(df_losses.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61bd7c5",
   "metadata": {},
   "source": [
    "## Generating sequences with the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
